{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# store information of a competition in one table: Competition\n",
    "\n",
    "<table>\n",
    "  <thead>\n",
    "    <tr>\n",
    "      <th>Datafield</th>\n",
    "      <th>Description</th>\n",
    "    </tr>\n",
    "  <tbody>\n",
    "    <tr>\n",
    "      <td>id</td>\n",
    "      <td>unique identifier for competition</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td>name</td>\n",
    "      <td>competition name</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td>start_time</td>\n",
    "      <td>start time of competition</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td>end_time</td>\n",
    "      <td>end time of competition</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td>prize</td>\n",
    "      <td>prize for winners</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td>entries</td>\n",
    "      <td>{{entry_id: xx, user_id: xx, entry_url:xxx, time:xxx}, {entry_id: xx, user_id: xx, entry_url:xxx, time:xxx}, ...}</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td>winners</td>\n",
    "      <td>[entry_id, entry_id]</td>\n",
    "    </tr>\n",
    "\n",
    "  </tbody>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import warnings\n",
    "from datetime import datetime\n",
    "warnings.filterwarnings('ignore')\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape(competition_url):\n",
    "\n",
    "\n",
    "    kv = {'user-agent': 'Mozilla/5.0'}\n",
    "    r = requests.get(competition_url, headers=kv, timeout=30)\n",
    "    r.raise_for_status()\n",
    "    r.status_code\n",
    "    r.encoding = r.apparent_encoding\n",
    "\n",
    "\n",
    "    # In[4]:\n",
    "\n",
    "\n",
    "    soup = BeautifulSoup(r.text, 'html.parser')\n",
    "\n",
    "\n",
    "    # In[5]:\n",
    "\n",
    "\n",
    "    ## title\n",
    "    result = soup.find(name='h1', attrs=\"heading heading--h1 heading--no-margin\")\n",
    "    if result:\n",
    "        title = result.text\n",
    "    else:\n",
    "        title = None\n",
    "\n",
    "    title\n",
    "\n",
    "\n",
    "    # In[6]:\n",
    "\n",
    "\n",
    "    results = soup.find(name='div', attrs='inline-page')\n",
    "\n",
    "\n",
    "    # In[7]:\n",
    "\n",
    "\n",
    "    headers = results.findAll(\"p\", \"heading heading--size4 heading--no-margin\")\n",
    "    headers\n",
    "\n",
    "\n",
    "    # ## 2. get brief page\n",
    "\n",
    "    # In[8]:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # In[9]:\n",
    "\n",
    "\n",
    "    PATH = \"C:\\Program Files (x86)\\chromedriver.exe\" \n",
    "\n",
    "\n",
    "    # In[10]:\n",
    "\n",
    "\n",
    "    driver = webdriver.Chrome(PATH)\n",
    "    driver.get(competition_url + '/brief')\n",
    "\n",
    "\n",
    "    # In[11]:\n",
    "\n",
    "\n",
    "    background_information = driver.find_element(By.XPATH, '//div[@id=\"section-backgroundInformation\"]/div')\n",
    "    name_to_incorporate = background_information.find_element(By.XPATH, '//div[@id=\"element-backgroundInformation-logoBusinessName\"]/div/div/div').text\n",
    "    slogan_to_incorporate = background_information.find_element(By.XPATH, '//div[@id=\"element-backgroundInformation-slogan\"]/div/div/div/div').text\n",
    "    description = background_information.find_element(By.XPATH, '//div[@id=\"element-backgroundInformation-targetAudience\"]/div/div/div').text\n",
    "    industry = background_information.find_element(By.XPATH, '//div[@id=\"element-backgroundInformation-industry\"]/div/div/div').text\n",
    "\n",
    "\n",
    "    # In[12]:\n",
    "\n",
    "\n",
    "    visual_style = driver.find_element(By.XPATH, '//div[@id=\"section-visualStyle\"]/div')\n",
    "    logo_types = driver.find_element(By.XPATH, '/html/body/div[1]/div[1]/div[3]/div/div/div[2]/div/div[1]/div/div/div/div/div[2]/div/div/div[1]/div/div/div/div').text\n",
    "    logo_to_be_used = driver.find_element(By.XPATH, '/html/body/div[1]/div[1]/div[3]/div/div/div[2]/div/div[1]/div/div/div/div/div[2]/div/div/div[2]/div/div/div').text\n",
    "    colors_to_explore = visual_style.find_element(By.XPATH, '//*[@id=\"element-visualStyle-colorPreferences\"]/div/div/div').text.split('\\n')\n",
    "    other_color_requirements = visual_style.find_element(By.XPATH, '//div[@id=\"element-visualStyle-otherColorRequirements\"]/div/div/div').text\n",
    "    style_attributes = []\n",
    "    for i in range(7):\n",
    "        value = visual_style.find_element(By.XPATH, '//*[@id=\"element-visualStyle-styleAttributes\"]/div/div/div/div/div[' + str(i+1) + ']/div[2]/div/div').get_attribute(\"aria-valuenow\")\n",
    "        style_attributes.append(value)\n",
    "\n",
    "    try: \n",
    "        design_inspiration = visual_style.find_element(By.XPATH, '//*[@id=\"element-visualStyle-designExamples\"]/div/div/div/div/div')\n",
    "        design_inspiration_count = len(design_inspiration.find_elements(By.CLASS_NAME, \"matrix__item\"))\n",
    "    except: \n",
    "        design_inspiration_count = 0\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # In[13]:\n",
    "\n",
    "\n",
    "    references = visual_style.find_element(By.XPATH, '//div[@id=\"section-references\"]')\n",
    "    attachments = references.find_element(By.XPATH, '//*[@id=\"element-references-attachments\"]/div/div/div').text\n",
    "    if attachments == 'No files':\n",
    "        attachments_count = 0\n",
    "    else:\n",
    "        attachments_count = len(references.find_elements(By.CLASS_NAME, \"matrix__item\"))\n",
    "    other_notes = references.find_element(By.XPATH, '//*[@id=\"element-references-notes\"]/div/div/div').text\n",
    "\n",
    "\n",
    "    # In[14]:\n",
    "\n",
    "\n",
    "    driver.close()\n",
    "\n",
    "\n",
    "    # ## 3. get participants and entry info\n",
    "\n",
    "    # In[15]:\n",
    "\n",
    "\n",
    "    kv = {'user-agent': 'Mozilla/5.0'}\n",
    "    url = competition_url + \"/entries\"\n",
    "\n",
    "    r = requests.get(url, headers=kv, timeout=30)\n",
    "    r.raise_for_status()\n",
    "    r.status_code\n",
    "    r.encoding = r.apparent_encoding\n",
    "\n",
    "\n",
    "    # In[16]:\n",
    "\n",
    "\n",
    "    soup = BeautifulSoup(r.text, 'html.parser')\n",
    "\n",
    "\n",
    "    # In[17]:\n",
    "\n",
    "\n",
    "    ## competition start date\n",
    "    text = str(soup.find(name='div', attrs=\"contest-header__price\"))\n",
    "    start_time = re.search('\"startDate\": .+,', text).group(0).replace('\"startDate\": \"', '')[:-2]\n",
    "    # print(start_time)\n",
    "\n",
    "\n",
    "    # ### winner id and entry id\n",
    "\n",
    "    # In[18]:\n",
    "\n",
    "\n",
    "    def make_soup(competition_url, page_number, active=True):\n",
    "\n",
    "        kv = {'user-agent': 'Mozilla/5.0'}\n",
    "        if active:\n",
    "            url = competition_url + \"/entries?filter=active&page=\" + str(page_number)\n",
    "        else: \n",
    "            url = competition_url + \"/entries?filter=non_active&page=\" + str(page_number)\n",
    "\n",
    "        r = requests.get(url, headers=kv, timeout=30)\n",
    "        r.raise_for_status()\n",
    "        r.status_code\n",
    "        r.encoding = r.apparent_encoding\n",
    "        soup = BeautifulSoup(r.text, 'html.parser')\n",
    "        return soup\n",
    "\n",
    "\n",
    "    # In[19]:\n",
    "\n",
    "\n",
    "    def get_winner_info(soup):\n",
    "\n",
    "        results = soup.findAll(name='div', attrs='entry-matrix__item matrix__item entry-winners')\n",
    "            \n",
    "        winner_entry_ids = []\n",
    "        for result in results:\n",
    "            winner_entry = result.find(name='div', attrs=\"entry entry--linked entry--zoom-linked\")\n",
    "            if winner_entry:\n",
    "                winner_entry_ids.append(winner_entry[\"id\"])\n",
    "                continue\n",
    "            winner_entry = result.find(name='div', attrs=\"entry\") # case if winner entry is deleted\n",
    "            if winner_entry:\n",
    "                winner_entry_ids.append(winner_entry[\"id\"])\n",
    "                continue\n",
    "        return winner_entry_ids\n",
    "\n",
    "\n",
    "    # In[20]:\n",
    "\n",
    "\n",
    "    page_number = 1\n",
    "    soup = make_soup(competition_url, page_number)\n",
    "    winner_entry_ids = get_winner_info(soup) \n",
    "    winner_entry_ids\n",
    "\n",
    "\n",
    "    # In[21]:\n",
    "\n",
    "\n",
    "    seeker = soup.find(name='span', attrs=\"display-name\").text\n",
    "    seeker\n",
    "\n",
    "\n",
    "    # In[22]:\n",
    "\n",
    "\n",
    "    entry_summary = soup.find(name='select', attrs=\"styled-select__select\")\n",
    "    entry_summary = entry_summary.text.replace('\\n', \"\")\n",
    "    entry_summary\n",
    "\n",
    "\n",
    "    # In[23]:\n",
    "\n",
    "\n",
    "    entry_count = int(re.search('All \\(\\d+\\)', entry_summary).group(0)[5: -1])\n",
    "    deleted_entry_count = re.search('Declined and withdrawn \\(\\d+\\)', entry_summary).group(0)[24:-1]\n",
    "    # print(entry_count, deleted_entry_count)\n",
    "\n",
    "\n",
    "    # In[24]:\n",
    "\n",
    "\n",
    "    def get_participant_and_entry_info(soup):\n",
    "\n",
    "        entry_matrix = soup.findAll(name='div', attrs='entry-matrix__item matrix__item')\n",
    "        for entry in entry_matrix:\n",
    "            result = entry.find(name='div', attrs='entry entry--linked entry--zoom-linked')\n",
    "            try:\n",
    "                participants_user_ids.append(result[\"data-user-id\"])\n",
    "                participants_entry_ids.append(result[\"id\"])\n",
    "                participants_entry_image_urls.append(result.find(name='a')[\"href\"])\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "\n",
    "    # In[25]:\n",
    "\n",
    "\n",
    "    def get_time(soup):\n",
    "        ## get time\n",
    "        # text = str(soup.find(name='div', attrs='entry-pane__results').contents)\n",
    "        text = str(soup.find(name='div', attrs='entry-pane__results'))\n",
    "        time = re.findall('\"timeCreatedString\":\".{20,30}\"', text)\n",
    "        if time:\n",
    "            time = list(map(lambda x: x.replace('\"timeCreatedString\":\"', '').replace('\",\"', \"\"), time))\n",
    "            return time\n",
    "\n",
    "        time = re.findall('timeCreatedString\\&quot\\;\\:&quot;.{20,30}\\&quot\\;', text)\n",
    "        if time:\n",
    "            time = list(map(lambda x: x[30: -6], time))\n",
    "            return time\n",
    "\n",
    "        \n",
    "    # In[26]:\n",
    "\n",
    "\n",
    "    def get_prize(soup):\n",
    "        ## get time\n",
    "        text = str(soup.find(name='div', attrs='contest-header contest-header--with-breadcrumbs').contents)\n",
    "        prize = re.search('\"prizeMoney\": \".{2,10}\"', text).group(0)\n",
    "        prize = prize.replace('\"prizeMoney\": \"', \"\").replace('\"', \"\")\n",
    "        return prize\n",
    "\n",
    "\n",
    "    # In[ ]:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # In[27]:\n",
    "\n",
    "\n",
    "    participants_user_ids = []\n",
    "    participants_entry_ids = []\n",
    "    participants_entry_image_urls = []\n",
    "    participants_submission_time = []\n",
    "    page_number = 1\n",
    "\n",
    "\n",
    "\n",
    "    while True:\n",
    "        if page_number == 1:\n",
    "            soup = make_soup(competition_url, page_number)\n",
    "            prize = get_prize(soup)\n",
    "            winner_entry_ids = get_winner_info(soup)\n",
    "            get_participant_and_entry_info(soup)\n",
    "            time = get_time(soup)\n",
    "            participants_submission_time += time\n",
    "            if len(time) < 36:\n",
    "                break\n",
    "        else:\n",
    "            soup = make_soup(competition_url, page_number)\n",
    "            get_participant_and_entry_info(soup)\n",
    "            time = get_time(soup)\n",
    "            participants_submission_time += time\n",
    "            if len(time) < 36:\n",
    "                break\n",
    "\n",
    "        # print(participants_user_ids)\n",
    "        # print(participants_entry_ids)\n",
    "        # print(participants_entry_image_urls)\n",
    "\n",
    "        page_number += 1\n",
    "\n",
    "\n",
    "    # In[28]:\n",
    "\n",
    "\n",
    "    participants_submission_time[:5]\n",
    "\n",
    "\n",
    "    # In[29]:\n",
    "\n",
    "\n",
    "    entries = []\n",
    "    for entry_id, participant_id, time, url in zip(participants_entry_ids, \n",
    "                                                    participants_user_ids, \n",
    "                                                participants_submission_time, \n",
    "                                                participants_entry_image_urls):\n",
    "        entries.append({'entry_id': entry_id, 'participant_id': participant_id, 'time': time, 'url':url})\n",
    "\n",
    "\n",
    "    # In[30]:\n",
    "\n",
    "\n",
    "    entries[:5]\n",
    "\n",
    "\n",
    "    # In[31]:\n",
    "\n",
    "\n",
    "    entries[-5:]\n",
    "\n",
    "\n",
    "    # In[ ]:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # ## 3. get participants and entry info for deleted entries\n",
    "\n",
    "    # In[32]:\n",
    "\n",
    "\n",
    "    def get_time_and_status_for_deleted_entry(entry_id):\n",
    "        entry_id = entry_id.split('-')[-1]\n",
    "        entry_url = competition_url + '/entries/' + entry_id\n",
    "        \n",
    "        ## make soup for entry page\n",
    "        kv = {'user-agent': 'Mozilla/5.0'}\n",
    "        r = requests.get(entry_url, headers=kv, timeout=30)\n",
    "        r.raise_for_status()\n",
    "        r.status_code\n",
    "        r.encoding = r.apparent_encoding\n",
    "        soup = BeautifulSoup(r.text, 'html.parser')\n",
    "\n",
    "        ## \n",
    "        script = str(soup.find('script', id=\"standalone-design-details-app-data\"))\n",
    "        time = re.search('\"timeCreatedString\":\".{20,30}\"', script).group(0)\n",
    "        time = time.replace('\"timeCreatedString\":\"', '').replace('\",\"', \"\")\n",
    "\n",
    "        status = 'deleted'\n",
    "        if re.search('\"status\":\"withdrawn\"', script):\n",
    "            status = 'withdrawn'\n",
    "        elif re.search('\"status\":\"eliminated\"', script):\n",
    "            status = 'declined'\n",
    "        \n",
    "\n",
    "        return time, status\n",
    "        \n",
    "\n",
    "\n",
    "    # In[33]:\n",
    "\n",
    "\n",
    "    def get_participant_and_entry_info_deleted_page(soup):\n",
    "\n",
    "        user_ids = []\n",
    "        entry_ids = []\n",
    "        status = []\n",
    "        submission_time = []\n",
    "        deleted_count = 0\n",
    "        withdrawn_count = 0\n",
    "        declined_count = 0\n",
    "\n",
    "        entry_matrix = soup.findAll(name='div', attrs='entry-matrix__item matrix__item')\n",
    "        \n",
    "        for entry in entry_matrix:\n",
    "            result = entry.find(name='div', attrs='entry')\n",
    "        # print(result)\n",
    "            try:\n",
    "                user_ids.append(result[\"data-user-id\"])\n",
    "                entry_ids.append(result[\"id\"])\n",
    "                time, s = get_time_and_status_for_deleted_entry(result[\"id\"])\n",
    "                submission_time.append(time)\n",
    "                status.append(s)\n",
    "\n",
    "                if s == 'deleted':\n",
    "                    deleted_count += 1\n",
    "                elif s == 'withdrawn':\n",
    "                    withdrawn_count += 1  \n",
    "                elif s == 'declined':\n",
    "                    declined_count += 1\n",
    "                    \n",
    "\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "        return user_ids, entry_ids, status, submission_time, deleted_count, withdrawn_count, declined_count\n",
    "\n",
    "\n",
    "    # In[34]:\n",
    "\n",
    "\n",
    "    deleted_participants_user_ids = []\n",
    "    deleted_participants_entry_ids = []\n",
    "    status_all = []\n",
    "    submission_time_all = []\n",
    "    total_deleted_count = 0\n",
    "    total_withdrawn_count = 0\n",
    "    total_declined_count = 0\n",
    "\n",
    "    page_number = 1\n",
    "    winner_number = len(winner_entry_ids)\n",
    "\n",
    "\n",
    "    while True:\n",
    "        # print(page_number)\n",
    "        if page_number == 1:\n",
    "            soup = make_soup(competition_url, page_number, active=False)\n",
    "            winner_entry_ids = get_winner_info(soup)\n",
    "            user_ids, entry_ids, status, submission_time, deleted_count, withdrawn_count, declined_count = get_participant_and_entry_info_deleted_page(soup)\n",
    "            deleted_participants_user_ids += user_ids\n",
    "            deleted_participants_entry_ids += entry_ids\n",
    "            status_all += status\n",
    "            submission_time_all += submission_time\n",
    "            total_deleted_count += deleted_count\n",
    "            total_withdrawn_count += withdrawn_count\n",
    "            total_declined_count += declined_count\n",
    "            if len(user_ids) < 36:\n",
    "                break\n",
    "        else:\n",
    "            soup = make_soup(competition_url, page_number, active=False)\n",
    "            user_ids, entry_ids, status, submission_time, deleted_count, withdrawn_count, declined_count = get_participant_and_entry_info_deleted_page(soup)\n",
    "            deleted_participants_user_ids += user_ids\n",
    "            deleted_participants_entry_ids += entry_ids\n",
    "            status_all += status\n",
    "            submission_time_all += submission_time\n",
    "            total_deleted_count += deleted_count\n",
    "            total_withdrawn_count += withdrawn_count\n",
    "            total_declined_count += declined_count\n",
    "            if len(user_ids) < 36:\n",
    "                break\n",
    "\n",
    "        page_number += 1\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    deleted_entries = []\n",
    "    for entry_id, participant_id, status, submission_time in zip(deleted_participants_entry_ids, \n",
    "                                        deleted_participants_user_ids, status_all, submission_time_all):\n",
    "        deleted_entries.append({'entry_id': entry_id, 'participant_id': participant_id, 'status': status, 'time': submission_time})\n",
    "\n",
    "\n",
    "    # ## output to file\n",
    "\n",
    "    # In[40]:\n",
    "\n",
    "\n",
    "    df_competition_description = pd.DataFrame({'title': [title], 'url': competition_url, 'seeker': seeker, 'start_time': start_time,\n",
    "                                                'name_to_incorporate': [name_to_incorporate], 'slogan_to_incorporate': [slogan_to_incorporate], \n",
    "                                                'description': [description], 'industry': [industry],  \n",
    "                                                'logo_types': [logo_types],\n",
    "                                                'logo_to_be_used': [logo_to_be_used],\n",
    "                                                'colors_to_explore': [colors_to_explore], \n",
    "                                                'other_color_requirements': [other_color_requirements], \n",
    "                                                'style_attributes': [style_attributes], \n",
    "                                                'design_inspiration_count': [design_inspiration_count],\n",
    "                                                'attachments_count': [attachments_count], \n",
    "                                                'other_notes': [other_notes],\n",
    "                                                'entry_count': entry_count, 'deleted_entry_count': deleted_entry_count,\n",
    "                                                'deleted': total_deleted_count, 'withdrawn': total_withdrawn_count, \n",
    "                                                'declined': total_declined_count, \n",
    "                                                'prize': prize, 'entries': [entries],\n",
    "                                                'deleted_entries': [deleted_entries], 'winners': [winner_entry_ids]})\n",
    "\n",
    "\n",
    "\n",
    "    return df_competition_description\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## output to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 52/52 [5:25:35<00:00, 375.69s/it]  \n"
     ]
    }
   ],
   "source": [
    "urls = pd.read_csv('href_2022-03-05_difference.csv')\n",
    "urls = urls['href'].values\n",
    "\n",
    "results = pd.DataFrame({})\n",
    "counter = 0\n",
    "\n",
    "for url in tqdm(urls):\n",
    "    counter += 1 \n",
    "    try:\n",
    "        df = scrape(url)\n",
    "        results = pd.concat([results, df])\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.to_csv('contest_data_' + str(datetime.now().date()) + '.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "5af03e6f6884cdcbf495aa98cbd8d63d5d0ed05506dbd026ba0a4981213ae30b"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
