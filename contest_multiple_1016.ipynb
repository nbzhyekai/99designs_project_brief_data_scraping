{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# store information of a competition in one table: Competition\n",
    "\n",
    "<table>\n",
    "  <thead>\n",
    "    <tr>\n",
    "      <th>Datafield</th>\n",
    "      <th>Description</th>\n",
    "    </tr>\n",
    "  <tbody>\n",
    "    <tr>\n",
    "      <td>id</td>\n",
    "      <td>unique identifier for competition</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td>name</td>\n",
    "      <td>competition name</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td>start_time</td>\n",
    "      <td>start time of competition</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td>end_time</td>\n",
    "      <td>end time of competition</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td>prize</td>\n",
    "      <td>prize for winners</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td>entries</td>\n",
    "      <td>{{entry_id: xx, user_id: xx, entry_url:xxx, time:xxx}, {entry_id: xx, user_id: xx, entry_url:xxx, time:xxx}, ...}</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td>winners</td>\n",
    "      <td>[entry_id, entry_id]</td>\n",
    "    </tr>\n",
    "\n",
    "  </tbody>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape(competition_url):\n",
    "\n",
    "\n",
    "    kv = {'user-agent': 'Mozilla/5.0'}\n",
    "    r = requests.get(competition_url, headers=kv, timeout=30)\n",
    "    r.raise_for_status()\n",
    "    r.status_code\n",
    "    r.encoding = r.apparent_encoding\n",
    "\n",
    "\n",
    "    # In[4]:\n",
    "\n",
    "\n",
    "    soup = BeautifulSoup(r.text, 'html.parser')\n",
    "\n",
    "\n",
    "    # In[5]:\n",
    "\n",
    "\n",
    "    ## title\n",
    "    result = soup.find(name='h1', attrs=\"heading heading--h1 heading--no-margin\")\n",
    "    if result:\n",
    "        title = result.text\n",
    "    else:\n",
    "        title = None\n",
    "\n",
    "    title\n",
    "\n",
    "\n",
    "    # In[6]:\n",
    "\n",
    "\n",
    "    results = soup.find(name='div', attrs='inline-page')\n",
    "\n",
    "    headers = results.findAll(\"p\", \"heading heading--size4 heading--no-margin\")\n",
    "    descriptions = results.findAll(\"p\", \"paragraph\")\n",
    "\n",
    "    summary = \"\"\n",
    "    company_name = \"\"\n",
    "    vision = \"\"\n",
    "\n",
    "    for header, description in zip(headers, descriptions):\n",
    "        if header.text == \"Summary\":\n",
    "            summary = description.text\n",
    "        elif header.text == \"Company name\":\n",
    "            company_name = description.text\n",
    "        elif header.text == \"What's your vision?\":\n",
    "            vision = description.text\n",
    "\n",
    "\n",
    "\n",
    "    kv = {'user-agent': 'Mozilla/5.0'}\n",
    "    url = competition_url + \"/entries\"\n",
    "\n",
    "    r = requests.get(url, headers=kv, timeout=30)\n",
    "    r.raise_for_status()\n",
    "    r.status_code\n",
    "    r.encoding = r.apparent_encoding\n",
    "\n",
    "\n",
    "    # In[10]:\n",
    "\n",
    "\n",
    "    soup = BeautifulSoup(r.text, 'html.parser')\n",
    "\n",
    "\n",
    "    # In[11]:\n",
    "\n",
    "\n",
    "    ## competition start date\n",
    "    text = str(soup.find(name='div', attrs=\"contest-header__price\"))\n",
    "    start_time = re.search('\"startDate\": .+,', text).group(0).replace('\"startDate\": \"', '')[:-2]\n",
    "\n",
    "\n",
    "\n",
    "    # ## 2. get participants and entry info\n",
    "\n",
    "    # ### winner id and entry id\n",
    "\n",
    "    # In[12]:\n",
    "\n",
    "\n",
    "    def make_soup(competition_url, page_number, active=True):\n",
    "\n",
    "        kv = {'user-agent': 'Mozilla/5.0'}\n",
    "        if active:\n",
    "            url = competition_url + \"/entries?filter=active&page=\" + str(page_number)\n",
    "        else: \n",
    "            url = competition_url + \"/entries?filter=non_active&page=\" + str(page_number)\n",
    " \n",
    "\n",
    "        r = requests.get(url, headers=kv, timeout=30)\n",
    "        r.raise_for_status()\n",
    "        r.status_code\n",
    "        r.encoding = r.apparent_encoding\n",
    "        soup = BeautifulSoup(r.text, 'html.parser')\n",
    "        return soup\n",
    "\n",
    "\n",
    "    # In[13]:\n",
    "\n",
    "\n",
    "    def get_winner_info(soup):\n",
    "\n",
    "        results = soup.findAll(name='div', attrs='entry-matrix__item matrix__item entry-winners')\n",
    "            \n",
    "        winner_entry_ids = []\n",
    "        for result in results:\n",
    "            winner_entry = result.find(name='div', attrs=\"entry entry--linked entry--zoom-linked\")\n",
    "            if winner_entry:\n",
    "                winner_entry_ids.append(winner_entry[\"id\"])\n",
    "                continue\n",
    "            winner_entry = result.find(name='div', attrs=\"entry\") # case if winner entry is deleted\n",
    "            if winner_entry:\n",
    "                winner_entry_ids.append(winner_entry[\"id\"])\n",
    "                continue\n",
    "        return winner_entry_ids\n",
    "\n",
    "\n",
    "    # In[14]:\n",
    "\n",
    "\n",
    "    page_number = 1\n",
    "    soup = make_soup(competition_url, page_number)\n",
    "    winner_entry_ids = get_winner_info(soup) \n",
    "    winner_entry_ids\n",
    "\n",
    "\n",
    "    # In[15]:\n",
    "\n",
    "\n",
    "    seeker = soup.find(name='span', attrs=\"display-name\").text\n",
    "    seeker\n",
    "\n",
    "\n",
    "    # In[16]:\n",
    "\n",
    "\n",
    "    entry_summary = soup.find(name='select', attrs=\"styled-select__select\")\n",
    "    entry_summary = entry_summary.text.replace('\\n', \"\")\n",
    "    entry_summary\n",
    "\n",
    "\n",
    "    # In[17]:\n",
    "\n",
    "\n",
    "    entry_count = int(re.search('All \\(\\d+\\)', entry_summary).group(0)[5: -1])\n",
    "    deleted_entry_count = re.search('Declined and withdrawn \\(\\d+\\)', entry_summary).group(0)[24:-1]\n",
    "\n",
    "\n",
    "    # In[18]:\n",
    "\n",
    "\n",
    "    def get_participant_and_entry_info(soup):\n",
    "\n",
    "        entry_matrix = soup.findAll(name='div', attrs='entry-matrix__item matrix__item')\n",
    "        for entry in entry_matrix:\n",
    "            result = entry.find(name='div', attrs='entry entry--linked entry--zoom-linked')\n",
    "            try:\n",
    "                participants_user_ids.append(result[\"data-user-id\"])\n",
    "                participants_entry_ids.append(result[\"id\"])\n",
    "                participants_entry_image_urls.append(result.find(name='a')[\"href\"])\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "\n",
    "    # In[19]:\n",
    "\n",
    "\n",
    "    def get_time(soup):\n",
    "        ## get time\n",
    "        # text = str(soup.find(name='div', attrs='entry-pane__results').contents)\n",
    "        text = str(soup.find(name='div', attrs='entry-pane__results'))\n",
    "        time = re.findall('\"timeCreatedString\":\".{20,30}\"', text)\n",
    "        time = list(map(lambda x: x.replace('\"timeCreatedString\":\"', '').replace('\",\"', \"\"), time))\n",
    "        return time\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def get_prize(soup):\n",
    "        ## get time\n",
    "        text = str(soup.find(name='div', attrs='contest-header contest-header--with-breadcrumbs').contents)\n",
    "        prize = re.search('\"prizeMoney\": \".{2,10}\"', text).group(0)\n",
    "        prize = prize.replace('\"prizeMoney\": \"', \"\").replace('\"', \"\")\n",
    "        return prize\n",
    "\n",
    "\n",
    "\n",
    "    participants_user_ids = []\n",
    "    participants_entry_ids = []\n",
    "    participants_entry_image_urls = []\n",
    "    participants_submission_time = []\n",
    "    page_number = 1\n",
    "\n",
    "\n",
    "\n",
    "    while True:\n",
    "\n",
    "        if page_number == 1:\n",
    "            soup = make_soup(competition_url, page_number)\n",
    "            prize = get_prize(soup)\n",
    "            winner_entry_ids = get_winner_info(soup)\n",
    "            get_participant_and_entry_info(soup)\n",
    "            time = get_time(soup)\n",
    "            participants_submission_time += time\n",
    "            if \"entry-1\" in participants_entry_ids or len(time) < 36:\n",
    "                break\n",
    "        else:\n",
    "            soup = make_soup(competition_url, page_number)\n",
    "            get_participant_and_entry_info(soup)\n",
    "            time = get_time(soup)\n",
    "            participants_submission_time += time\n",
    "            if \"entry-1\" in participants_entry_ids or len(time) < 36:\n",
    "                break\n",
    "\n",
    "        page_number += 1\n",
    "\n",
    "\n",
    "    # In[22]:\n",
    "\n",
    "\n",
    "    participants_submission_time[:5]\n",
    "\n",
    "\n",
    "    # In[23]:\n",
    "\n",
    "\n",
    "    entries = []\n",
    "    for entry_id, participant_id, time, url in zip(participants_entry_ids, \n",
    "                                                    participants_user_ids, \n",
    "                                                participants_submission_time, \n",
    "                                                participants_entry_image_urls):\n",
    "        entries.append({'entry_id': entry_id, 'participant_id': participant_id, 'time': time, 'url':url})\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # ## 3. get participants and entry info for deleted entries\n",
    "\n",
    "    # In[26]:\n",
    "\n",
    "\n",
    "    def get_participant_and_entry_info_deleted_page(soup):\n",
    "\n",
    "        user_ids = []\n",
    "        entry_ids = []\n",
    "        status = []\n",
    "        deleted_count = 0\n",
    "        withdrawn_count = 0\n",
    "        declined_count = 0\n",
    "\n",
    "        entry_matrix = soup.findAll(name='div', attrs='entry-matrix__item matrix__item')\n",
    "        \n",
    "        for entry in entry_matrix:\n",
    "            result = entry.find(name='div', attrs='entry')\n",
    "        # print(result)\n",
    "            try:\n",
    "                user_ids.append(result[\"data-user-id\"])\n",
    "                entry_ids.append(result[\"id\"])\n",
    "                s = ''\n",
    "                for tag in entry.findAll('div', \"entry-status-overlay\"):\n",
    "                    attributes = tag.attrs\n",
    "                    if attributes['data-entry-status'] == 'deleted' and 'data-hidden' not in attributes:\n",
    "                        s = 'deleted'\n",
    "                        deleted_count += 1\n",
    "                        break\n",
    "                    if attributes['data-entry-status'] == 'withdrawn' and 'data-hidden' not in attributes:\n",
    "                        s = 'withdrawn'\n",
    "                        withdrawn_count += 1\n",
    "                        break\n",
    "                    if attributes['data-entry-status'] == 'declined' and 'data-hidden' not in attributes:\n",
    "                        s = 'declined' \n",
    "                        declined_count += 1\n",
    "                        break\n",
    "                \n",
    "                status.append(s)\n",
    "\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "        return user_ids, entry_ids, status, deleted_count, withdrawn_count, declined_count\n",
    "\n",
    "    # In[27]:\n",
    "\n",
    "    deleted_participants_user_ids = []\n",
    "    deleted_participants_entry_ids = []\n",
    "    status_all = []\n",
    "    total_deleted_count = 0\n",
    "    total_withdrawn_count = 0\n",
    "    total_declined_count = 0\n",
    "\n",
    "    page_number = 1\n",
    "    winner_number = len(winner_entry_ids)\n",
    "\n",
    "\n",
    "    while True:\n",
    "        # print(page_number)\n",
    "        if page_number == 1:\n",
    "            soup = make_soup(competition_url, page_number, active=False)\n",
    "            winner_entry_ids = get_winner_info(soup)\n",
    "            user_ids, entry_ids, status, deleted_count, withdrawn_count, declined_count = get_participant_and_entry_info_deleted_page(soup)\n",
    "            deleted_participants_user_ids += user_ids\n",
    "            deleted_participants_entry_ids += entry_ids\n",
    "            status_all += status\n",
    "            total_deleted_count += deleted_count\n",
    "            total_withdrawn_count += withdrawn_count\n",
    "            total_declined_count += declined_count\n",
    "            if \"entry-1\" in entry_ids or len(user_ids) < 36:\n",
    "                break\n",
    "        else:\n",
    "            soup = make_soup(competition_url, page_number, active=False)\n",
    "            user_ids, entry_ids, status, deleted_count, withdrawn_count, declined_count = get_participant_and_entry_info_deleted_page(soup)\n",
    "            deleted_participants_user_ids += user_ids\n",
    "            deleted_participants_entry_ids += entry_ids\n",
    "            status_all += status\n",
    "            total_deleted_count += deleted_count\n",
    "            total_withdrawn_count += withdrawn_count\n",
    "            total_declined_count += declined_count\n",
    "            if \"entry-1\" in entry_ids or len(user_ids) < 36:\n",
    "                break\n",
    "\n",
    "        page_number += 1\n",
    "\n",
    "    # In[29]:\n",
    "\n",
    "\n",
    "    deleted_entries = []\n",
    "    for entry_id, participant_id, status in zip(deleted_participants_entry_ids, \n",
    "                                                    deleted_participants_user_ids, status_all):\n",
    "        deleted_entries.append({'entry_id': entry_id, 'participant_id': participant_id, 'status': status})\n",
    "\n",
    "\n",
    "    df_competition_description = pd.DataFrame({'title': [title], 'url': competition_url, 'seeker': seeker, 'summary': [summary], \n",
    "                                                'company_name': [company_name], \n",
    "                                                'vision': [vision],  'start_time': start_time, \n",
    "                                                'entry_count': entry_count, 'deleted_entry_count': deleted_entry_count,\n",
    "                                                'deleted': total_deleted_count, 'withdrawn': total_withdrawn_count, \n",
    "                                                'declined': total_declined_count, \n",
    "                                                'prize': prize, 'entries': [entries],\n",
    "                                                'deleted_entries': [deleted_entries], 'winners': [winner_entry_ids]})\n",
    "\n",
    "\n",
    "    return df_competition_description\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## output to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1500/1500 [4:50:13<00:00, 11.61s/it]\n"
     ]
    }
   ],
   "source": [
    "urls = pd.read_csv('competition_data_1013/href.csv')\n",
    "urls = urls['href'][:]\n",
    "\n",
    "results = pd.DataFrame({})\n",
    "counter = 0\n",
    "\n",
    "for url in tqdm(urls):\n",
    "    counter += 1 \n",
    "    try:\n",
    "        df = scrape(url)\n",
    "        results = pd.concat([results, df])\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.to_csv('competition_data_1016/first1500-v2.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "5af03e6f6884cdcbf495aa98cbd8d63d5d0ed05506dbd026ba0a4981213ae30b"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
